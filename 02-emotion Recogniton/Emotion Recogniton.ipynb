{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4803fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf73f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ea10fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf569519",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16a9b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aneas\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a383f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aneas\\AppData\\Local\\Temp\\ipykernel_4896\\2005855100.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 648s 1s/step - loss: 1.8055 - accuracy: 0.2568\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 658s 1s/step - loss: 1.6455 - accuracy: 0.3569\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 896s 2s/step - loss: 1.5331 - accuracy: 0.4084\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 651s 1s/step - loss: 1.4580 - accuracy: 0.4433\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 659s 1s/step - loss: 1.3933 - accuracy: 0.4691\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 743s 2s/step - loss: 1.3370 - accuracy: 0.4929\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 787s 2s/step - loss: 1.2921 - accuracy: 0.5099\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 432s 963ms/step - loss: 1.2587 - accuracy: 0.5223\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 211s 470ms/step - loss: 1.2216 - accuracy: 0.5392\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 314s 699ms/step - loss: 1.1894 - accuracy: 0.5527\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 315s 703ms/step - loss: 1.1564 - accuracy: 0.5640\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 313s 697ms/step - loss: 1.1218 - accuracy: 0.5789\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 315s 703ms/step - loss: 1.0963 - accuracy: 0.5915\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 314s 702ms/step - loss: 1.0717 - accuracy: 0.6007\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 312s 697ms/step - loss: 1.0435 - accuracy: 0.6083\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 314s 701ms/step - loss: 1.0197 - accuracy: 0.6207\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 312s 697ms/step - loss: 0.9926 - accuracy: 0.6348\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 315s 703ms/step - loss: 0.9733 - accuracy: 0.6382\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 313s 700ms/step - loss: 0.9448 - accuracy: 0.6501\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 312s 697ms/step - loss: 0.9161 - accuracy: 0.6618\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 312s 697ms/step - loss: 0.8901 - accuracy: 0.6710\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 316s 704ms/step - loss: 0.8704 - accuracy: 0.6791\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 312s 697ms/step - loss: 0.8484 - accuracy: 0.6889\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 313s 700ms/step - loss: 0.8192 - accuracy: 0.7011\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 312s 697ms/step - loss: 0.8013 - accuracy: 0.7072\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 315s 705ms/step - loss: 0.7673 - accuracy: 0.7205\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 313s 697ms/step - loss: 0.7472 - accuracy: 0.7269\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 314s 701ms/step - loss: 0.7224 - accuracy: 0.7368\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 313s 698ms/step - loss: 0.7029 - accuracy: 0.7437\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 317s 705ms/step - loss: 0.6798 - accuracy: 0.7549\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 313s 699ms/step - loss: 0.6543 - accuracy: 0.7619\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 314s 700ms/step - loss: 0.6304 - accuracy: 0.7689\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 312s 697ms/step - loss: 0.6126 - accuracy: 0.7760\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 314s 701ms/step - loss: 0.5858 - accuracy: 0.7868\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 314s 701ms/step - loss: 0.5758 - accuracy: 0.7898\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 312s 697ms/step - loss: 0.5520 - accuracy: 0.8019\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 315s 703ms/step - loss: 0.5309 - accuracy: 0.8062\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 314s 699ms/step - loss: 0.5146 - accuracy: 0.8148\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 312s 698ms/step - loss: 0.4973 - accuracy: 0.8193\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 314s 697ms/step - loss: 0.4825 - accuracy: 0.8260\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 315s 704ms/step - loss: 0.4594 - accuracy: 0.8342\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 314s 698ms/step - loss: 0.4424 - accuracy: 0.8392\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 315s 703ms/step - loss: 0.4268 - accuracy: 0.8467\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 318s 709ms/step - loss: 0.4155 - accuracy: 0.8538\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 306s 681ms/step - loss: 0.4103 - accuracy: 0.8525\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 218s 487ms/step - loss: 0.3896 - accuracy: 0.8610\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 220s 490ms/step - loss: 0.3798 - accuracy: 0.8651\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 218s 486ms/step - loss: 0.3745 - accuracy: 0.8647\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 219s 488ms/step - loss: 0.3533 - accuracy: 0.8744\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 221s 494ms/step - loss: 0.3451 - accuracy: 0.8795\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1121ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "emotion_model.save_weights('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e97297",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5923c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model/emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54fbd23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "emotion_model.load_weights(\"model/emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1976b04",
   "metadata": {},
   "source": [
    "## Web Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b06b15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911680e9",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563d582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 978ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(r\"emotion.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector = cv2.CascadeClassifier(r'haarcascades/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccd110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
